% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{bert_approximate_distribution}
\alias{bert_approximate_distribution}
\title{Get a string representation of the current object.
Â¶
A post-hoc approximation of topic distributions across documents.}
\usage{
bert_approximate_distribution(
  obj,
  docs,
  window = 4,
  stride = 1,
  min_similarity = 0.1,
  batch_size = 1000,
  padding = FALSE,
  use_embedding_model = FALSE,
  calculate_tokens = FALSE,
  separator = " "
)
}
\arguments{
\item{obj}{Topic Model Object}

\item{docs}{Vector of Documents}

\item{window}{Size of the moving window which indicates the number of tokens being considered.  Default `4`}

\item{stride}{How far the window should move at each step.  Default `1`.}

\item{min_similarity}{The minimum similarity of a document's tokenset with respect to the topics.  Default `.1`}

\item{batch_size}{The number of documents to process at a time. If None, then all documents are processed at once. NOTE: With a large number of documents, it is not advised to process all documents at once.    Default `1000`}

\item{padding}{Whether to pad the beginning and ending of a document with empty tokens.  Default `FALSE`}

\item{use_embedding_model}{Whether to use the topic model's embedding model to calculate the similarity between tokensets and topics instead of using c-TF-IDF.  Default `FALSE`}

\item{calculate_tokens}{Calculate the similarity of tokens with all topics. NOTE: This is computation-wise more expensive and can require more memory. Using this over batches of documents might be preferred.  Default `FALSE`}

\item{separator}{The separator used to merge tokens into tokensets.}
}
\description{
In order to perform this approximation, each document is split into tokens according to the provided tokenizer in the CountVectorizer. Then, a sliding window is applied on each document creating subsets of the document. For example, with a window size of 3 and stride of 1, the sentence:
}
