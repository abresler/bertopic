% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bertopic.R
\name{bert_topic}
\alias{bert_topic}
\title{Initiate BERT Topic Model}
\usage{
bert_topic(
  language = "english",
  representation_model = NULL,
  top_n_words = 10L,
  use_key_phrase_vectorizer = F,
  use_sklearn_vectorizer = F,
  is_lower_case = T,
  n_gram_range = list(1L, 1L),
  keyphrase_ngram_range = list(1L, 1L),
  min_topic_size = 10L,
  umap_model = NULL,
  hdbscan_model = NULL,
  vectorizer_model = NULL,
  embedding_model = NULL,
  ctfidf_model = NULL,
  nr_topics = NULL,
  low_memory = F,
  exclude_stop_words = T,
  stopword_package_sources = NULL,
  extra_stop_words = NULL,
  calculate_probabilities = T,
  min_df = 1L,
  max_df = 1L,
  pos_pattern = "<J.*>*<N.*>+",
  seed_topic_list = NULL,
  verbose = T,
  vocabulary = NULL
)
}
\arguments{
\item{language}{The main language used in your documents. The default sentence-transformers model for "english" is all-MiniLM-L6-v2. For a full overview of supported languages see bertopic.backend.languages. Select "multilingual" to load in the paraphrase-multilingual-MiniLM-L12-v2 sentence-tranformers model that supports 50+ languages. Default `english`}

\item{representation_model}{}

\item{top_n_words}{The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words.  Default to `10L`}

\item{use_key_phrase_vectorizer}{if `TRUE` uses a keyphrase vectorizer}

\item{is_lower_case}{if `TRUE` is lowercase}

\item{n_gram_range}{The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. Default to `list(1L, 1L)`}

\item{min_topic_size}{The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. NOTE: This param will not be used if you are not using HDBSCAN. Default 10}

\item{umap_model}{Pass in a UMAP model to be used instead of the default. NOTE: You can also pass in any dimensionality reduction algorithm as long as it has .fit and .transform functions.}

\item{hdbscan_model}{Pass in a hdbscan.HDBSCAN model to be used instead of the default NOTE: You can also pass in any clustering algorithm as long as it has .fit and .predict functions along with the .labels_ variable. Default `NULL`}

\item{vectorizer_model}{Pass in a custom CountVectorizer instead of the default model. Default `NULL`}

\item{embedding_model}{type of embedding model - either an object or `NULL` options include \itemize{
\item \href{https://www.sbert.net/docs/pretrained_models.html}{sbert}
} and it defaults to `all-MiniLM-L6-v2`}

\item{ctfidf_model}{Pass in a custom ClassTfidfTransformer instead of the default model. Default `NULL`}

\item{nr_topics}{Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use `auto` to automatically reduce topics using HDBSCAN.}

\item{low_memory}{Sets UMAP low memory to True to make sure less memory is used. NOTE: This is only used in UMAP. For example, if you use PCA instead of UMAP this parameter will not be used. Default `FALSE`}

\item{exclude_stop_words}{if `TRUE` excludes base stop words}

\item{stopword_package_sources}{options if not `NULL` `c("snowball", "stopwords-iso", "smart", "nltk")`}

\item{extra_stop_words}{vector of other stopwords}

\item{calculate_probabilities}{Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method visualize_probabilities.  Default to `FALSE`}

\item{min_df}{During fitting ignore keyphrases that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.  Default `1L`}

\item{max_df}{During fitting ignore keyphrases that have a document frequency strictly higher than the given threshold. Default `1L`}

\item{pos_pattern}{Position patter for keyphrase.  Defaults to `pos_pattern = "<J.*>*<N.*>+",`}

\item{seed_topic_list}{A list of seed words per topic to converge around.  Default is `NULL`}

\item{verbose}{if `TRUE` verbose output}
}
\value{
python object
}
\description{
Functions for unsupervised clustering algorithms.
}
\details{
\itemize{
\item \href{https://maartengr.github.io/BERTopic/api/bertopic.html}{berttopic}
}
}
\examples{
import_bertopic()
data <- sklearn::sk_datasets()
docs_all <- data$fetch_20newsgroups(subset = 'all', remove = c('headers', 'footers', 'quotes'))
docs <- docs_all["data"]
tm <- bert_topic()
topic_model <- tm$fit_transform(documents = docs)


}
