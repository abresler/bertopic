% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{sklearn_vectorizer}
\alias{sklearn_vectorizer}
\title{SKLearn Word Vectorizer}
\usage{
sklearn_vectorizer(
  obj = NULL,
  language = "english",
  ngram_range = list(1L, 1L),
  analyzer = "word",
  stopword_package_sources = NULL,
  strip_accents = NULL,
  exclude_stop_words = T,
  extra_stop_words = NULL,
  token_pattern = "(?u)\\\\b\\\\w\\\\w+\\\\b",
  vocabulary = NULL,
  is_lower_case = TRUE,
  max_df = 1,
  min_df = 1,
  max_features = NULL,
  binary = FALSE
)
}
\arguments{
\item{ngram_range}{list The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callab}

\item{analyzer}{Remove accents and perform other character normalization during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have a direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any characters. None (default) does nothing.}

\item{exclude_stop_words}{if `TRUE` excludes stopwords}

\item{extra_stop_words}{if `TRUE` other stopwords to exclude}

\item{token_pattern}{Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).}

\item{is_lower_case}{if `TRUE` all to lower case}

\item{max_df}{During fitting ignore keyphrases that have a document frequency strictly higher than the given threshold. Default `NULL`}

\item{min_df}{During fitting ignore keyphrases that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.  Default `NULL`}

\item{max_features}{If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.}

\item{binary}{If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.}
}
\description{
SKLearn Word Vectorizer
}
\examples{
vectorizer_model <- sklearn_vectorizer(ngram_range = list(1L, 3L))
docs <- c('This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?')
vectorizer_model$fit_transform(raw_documents = docs)
vectorizer_model$get_feature_names_out()
}
